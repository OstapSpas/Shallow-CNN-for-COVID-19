import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# –í–∏–∑–Ω–∞—á–∞—î–º–æ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª—ñ
class ShallowCNN(nn.Module):
    def __init__(self):
        super(ShallowCNN, self).__init__()

        # –û–¥–∏–Ω –∑–≥–æ—Ä—Ç–∫–æ–≤–∏–π —à–∞—Ä (–∑–≥—ñ–¥–Ω–æ –∑—ñ —Å—Ç–∞—Ç—Ç–µ—é)
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=2)

        # –ü—É–ª—ñ–Ω–≥ (–∑–≥—ñ–¥–Ω–æ –∑—ñ —Å—Ç–∞—Ç—Ç–µ—é)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –¥–ª—è FC-—à–∞—Ä—É
        self.flatten_size = self._get_flatten_size()

        # –ü–æ–≤–Ω–æ–∑–≤‚Äô—è–∑–Ω—ñ —à–∞—Ä–∏ (–∑–≥—ñ–¥–Ω–æ –∑—ñ —Å—Ç–∞—Ç—Ç–µ—é)
        self.fc1 = nn.Linear(self.flatten_size, 256)
        self.fc2 = nn.Linear(256, 3)

    def _get_flatten_size(self):
        """–í–∏–∑–Ω–∞—á–∞—î —Ä–æ–∑–º—ñ—Ä —Ç–µ–Ω–∑–æ—Ä–∞ –ø—ñ—Å–ª—è –∑–≥–æ—Ä—Ç–∫–∏"""
        x = torch.randn(1, 3, 224, 224)  # –¢–µ—Å—Ç–æ–≤–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        x = self.pool(F.relu(self.conv1(x)))
        return x.view(-1).shape[0]

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = torch.flatten(x, start_dim=1)
        x = F.relu(self.fc1(x))
        x = F.log_softmax(self.fc2(x), dim=1)
        return x

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ
model = ShallowCNN()

# –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ –º–æ–¥–µ–ª—å
model = ShallowCNN()
print(model)

# –§—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç
# –í–∏–∑–Ω–∞—á–∞—î–º–æ, —á–∏ —î GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# –û—Ç—Ä–∏–º—É—î–º–æ –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤
class_counts = torch.tensor([57, 234, 390], dtype=torch.float32)  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–ø–∞–¥–∫—ñ–≤ COVID, NORMAL, PNEUMONIA
weights = class_counts.sum() / class_counts  # –í–∞–≥–∏ —É –∑–≤–æ—Ä–æ—Ç–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
weights = weights / weights.sum()  # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –≤–∞–≥–∏
weights = weights.to(device)

# –û–Ω–æ–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç
criterion = nn.CrossEntropyLoss(weight=weights)

# –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä (SGD)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ, —â–æ –≤—Å–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞–ª–æ—Å—è –±–µ–∑ –ø–æ–º–∏–ª–æ–∫
print(optimizer)

# –®–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
dataset_base = "chest_xray"

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (–∑–º—ñ–Ω–∞ —Ä–æ–∑–º—ñ—Ä—É, –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤ —Ç–µ–Ω–∑–æ—Ä, –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
train_dataset = datasets.ImageFolder(root=f"{dataset_base}/train", transform=transform)
val_dataset = datasets.ImageFolder(root=f"{dataset_base}/val", transform=transform)
test_dataset = datasets.ImageFolder(root=f"{dataset_base}/test", transform=transform)


# DataLoader –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ, —Å–∫—ñ–ª—å–∫–∏ –∫–ª–∞—Å—ñ–≤ (–º–∞—î –±—É—Ç–∏ 3: NORMAL, PNEUMONIA, COVID)
print("–ö–ª–∞—Å–∏:", train_dataset.classes)

# –ü–µ—Ä–µ–≤—ñ—Ä–∏–º–æ —Ä–æ–∑–º—ñ—Ä –≤–∏–±—ñ—Ä–∫–∏
print(f"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")


# –ù–∞–≤—á–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
num_epochs = 10 # –ú–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏ –Ω–∞ –±—ñ–ª—å—à–µ, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ GPU, —è–∫—â–æ —î
model.to(device)

# –¶–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è
for epoch in range(num_epochs):
    model.train()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –Ω–∞–≤—á–∞–Ω–Ω—è
    running_loss = 0.0
    correct, total = 0, 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)  # –ü–µ—Ä–µ–º—ñ—â—É—î–º–æ –Ω–∞ GPU, —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π

        optimizer.zero_grad()  # –û–±–Ω—É–ª—è—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏

        outputs = model(images)  # –ü—Ä–æ–≥—ñ–Ω –≤–ø–µ—Ä–µ–¥
        loss = criterion(outputs, labels)  # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç
        loss.backward()  # –ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        optimizer.step()  # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥

        running_loss += loss.item()

        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_accuracy = 100 * correct / total
    avg_loss = running_loss / len(train_loader)

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
    model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞—î–º–æ –≤ —Ä–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏
    val_correct, val_total = 0, 0
    val_loss = 0.0

    with torch.no_grad():  # –í–∏–º–∏–∫–∞—î–º–æ –æ–±—Ä–∞—Ö—É–Ω–æ–∫ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_accuracy = 100 * val_correct / val_total
    avg_val_loss = val_loss / len(val_loader)

    print(
        f"Epoch [{epoch + 1}/{num_epochs}] | Loss: {avg_loss:.4f} | Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%")

print("–ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


# –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
model.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞—î–º–æ –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü—ñ–Ω–∫–∏
test_correct, test_total = 0, 0
all_preds, all_labels = [], []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        test_total += labels.size(0)
        test_correct += (predicted == labels).sum().item()

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

test_accuracy = 100 * test_correct / test_total
print(f"\nüéØ –¢–æ—á–Ω—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ: {test_accuracy:.2f}%")

# –ö–ª–∞—Å–∏
class_names = train_dataset.classes  # ['COVID', 'NORMAL', 'PNEUMONIA']

# –í–∏–≤—ñ–¥ Precision, Recall, F1-score
print("\nüîç Classification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names))

# –ú–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫
conf_matrix = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("–ü—Ä–µ–¥–±–∞—á–µ–Ω—ñ –∫–ª–∞—Å–∏")
plt.ylabel("–†–µ–∞–ª—å–Ω—ñ –∫–ª–∞—Å–∏")
plt.title("–ú–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫")
plt.show()

